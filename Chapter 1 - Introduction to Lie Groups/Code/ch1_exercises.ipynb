{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Exercises\n",
    "# \n",
    "We approach this notebook following the textbook content, as available in the notes.md markdown file. \n",
    "\n",
    "# Manifolds\n",
    "\n",
    "For convenience, we restate the definition here:\n",
    "### **Definition 1.1** - **$M$-Dimensional Manifold**\n",
    "An **$m$-dimensional manifold** is a set **$M$**, together with a countable collection of subsets **$U_{\\alpha} \\sub M$**, called ***coordinate charts***, and one-to-one functions **$\\Chi_\\alpha \\colon U_\\alpha \\mapsto V_\\alpha$** onto connected oopen subsets **$V_{\\alpha}\\sub \\R^m$**, called ***local coordinate maps***, which satisfy the following properties:\n",
    "\n",
    "*a)* The ***coordinate*** charts *cover* **$M$**:\n",
    "$$\\bigcup_{\\alpha} U_{\\alpha} = M$$\n",
    "\n",
    "*b)* On the overlap of any pair of coordinate charts, $U_{\\alpha}\\cap U_{\\beta}$ the composite map\n",
    "$$\n",
    "\\chi_{\\beta}\\circ \\chi_{\\alpha}^{-1}\\colon \\chi_{\\alpha}(\n",
    "    U_{\\alpha}\\cap U_{\\beta}\n",
    ") \\mapsto \\chi_{\\beta}(\n",
    "    U_{\\alpha}\\cap U_{\\beta}\n",
    ")\n",
    "$$\n",
    "\n",
    "is a smooth (***inifinitely differentiable***) function.\n",
    "\n",
    "*c)* If $x \\in U_{\\alpha}$ and $\\tilde x \\in U_{\\beta}$ are distinct points of **$M$**, then there exists open subsets $W\\sub V_{\\alpha}$, $\\tilde W \\sub V_{\\beta}$ with $\\chi_{\\alpha}(x)\\in W$, $\\chi_{\\beta}(\\tilde x)\\in \\tilde W$, satisfying\n",
    "$$\n",
    "\\chi_{\\alpha}^{-1}(W)\\cap\\chi_{\\beta}^{-1}(\\tilde W) = \\emptyset\n",
    "$$\n",
    "\n",
    "We'll proceed with some simple examples based on Definition 1.1, which will help us gain a deeper understanding of manifolds and their properties.\n",
    "\n",
    "## Example 1: $S^{1}, S^{2}$, and $S^{3}$\n",
    "\n",
    "### $S^{1} = \\{(x, y)\\colon x^2 + y^2 = 1\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An easy example to start with to explore the definition of a manifold is S^1, the circle. We can parameterize the circle\n",
    "# such that it can be defined in terms of a single parameter, theta, as follows:\n",
    "# x = cos(theta)\n",
    "# y = sin(theta)\n",
    "\n",
    "# The circle is a 1-dimensional manifold, so we can define it as a\n",
    "# 1-dimensional tensor. We'll use 1000 points to define the circle.\n",
    "theta = torch.linspace(0, 2 * torch.pi, 1000)\n",
    "x = torch.cos(theta)\n",
    "y = torch.sin(theta)\n",
    "\n",
    "# Create a figure with two subplots: x and y as functions of theta, and x plotted against y with an example right triangle\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "# Plot x and y as functions of theta\n",
    "axs[0].plot(theta, x, label='x')\n",
    "axs[0].plot(theta, y, label='y')\n",
    "axs[0].set_title('\\u03B8 vs. x and y')\n",
    "axs[0].set_xlabel('\\u03B8')\n",
    "axs[0].set_ylabel('x and y')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot x vs y and the right triangle with the corresponding angle \n",
    "axs[1].plot(x, y)\n",
    "axs[1].set_title('Manifold: $S^1$')\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "\n",
    "# Select the point attheta = pi/4 and plot the triangle\n",
    "example_theta = torch.tensor(torch.pi / 4.0)\n",
    "example_x = torch.cos(example_theta)\n",
    "example_y = torch.sin(example_theta)\n",
    "\n",
    "axs[1].plot([0, example_x], [0, 0], 'r',label='cos(\\u03B8)')                                      # x edge\n",
    "axs[1].plot([example_x, example_x], [0, example_y], 'g', label='sin(\\u03B8)')                                      # y edge\n",
    "axs[1].plot([0, example_x], [0, example_y], 'b', label='radius')                                  # hypotenuse\n",
    "axs[1].plot(example_x, example_y, 'ko', label='Point (cos(\\u03B8), sin(\\u03B8))')                 # point\n",
    "axs[1].annotate('\\u03B8', (0.1, 0), fontsize=12)                                                   # theta label\n",
    "axs[1].legend()\n",
    "\n",
    "# Set aspect ratio for the x vs y plot\n",
    "axs[1].set_aspect('equal', 'box')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $S^2 = \\{(x, y, z)\\colon x^2 + y^2 + z^2\\ = 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S^2 is a good example of a nontrivial two-dimensional manifold,\n",
    "# realized as a surface in three-dimensional space.\n",
    "\n",
    "# Let'a define this in terms of the subsets U_1 and U_2, which cover S^2.\n",
    "# U_1 is the upper hemisphere, and U_2 is the lower hemisphere.\n",
    "# We can define these subsets in terms of the following coordinate charts:\n",
    "# S2 = { (x, y, z) in R^3 | x^2 + y^2 + z^2 = 1 }\n",
    "# U_1 = S^2 \\ { (0, 0, 1) }\n",
    "# U_2 = S^2 \\ { (0, 0, -1) }\n",
    "\n",
    "tolerance = 1e-2\n",
    "S2 = torch.tensor([[x.item(), y.item(), z.item()] for x in torch.linspace(-1, 1, 50) for y in torch.linspace(-1, 1, 50) for z in torch.linspace(-1, 1, 50) if abs(x**2 + y**2 + z**2 - 1.0) < tolerance ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(S2[:, 0], S2[:, 1], S2[:, 2]);\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define the coordinate charts for U_1 and U_2.\n",
    "\n",
    "# U_1 = { (x, y, z) in S^2 | z != 1 }\n",
    "# U_2 = { (x, y, z) in S^2 | z != -1 }\n",
    "\n",
    "U1 = torch.tensor([[x.item(), y.item(), z.item()] for x in torch.linspace(-1, 1, 50) for y in torch.linspace(-1, 1, 50) for z in torch.linspace(-1, 1, 50) if abs(x**2 + y**2 + z**2 - 1.0) < tolerance and abs(z - 1.0) > tolerance])\n",
    "U2 = torch.tensor([[x.item(), y.item(), z.item()] for x in torch.linspace(-1, 1, 50) for y in torch.linspace(-1, 1, 50) for z in torch.linspace(-1, 1, 50) if abs(x**2 + y**2 + z**2 - 1.0) < tolerance and abs(z + 1.0) > tolerance])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that U_1 and U_2 are subsets of S^2.\n",
    "torch.allclose(U1.norm(dim=1), torch.ones(len(U1)), atol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(U2.norm(dim=1), torch.ones(len(U2)), atol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1 = S2[S2[:, 2] > -tolerance]\n",
    "U2 = S2[S2[:, 2] < tolerance]\n",
    "\n",
    "# Combine U1 and U2\n",
    "U_combined = torch.cat([U1, U2], dim=0)\n",
    "\n",
    "# Check if all points in S2 are covered by U1 and U2\n",
    "is_covered = np.all([any([torch.allclose(point, candidate, atol=tolerance) for candidate in U_combined]) for point in S2])\n",
    "\n",
    "print(\"U1 and U2 cover S^2:\", is_covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can plot the subsets U_1 and U_2 using the Axes3D class from the mpl_toolkits.mplot3d library.\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(U1[:, 0], U1[:, 1], U1[:, 2]);\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(U2[:, 0], U2[:, 1], U2[:, 2]);\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(torch.cat([U1[:, 0],U2[:, 0]]), torch.cat([U1[:, 1],U2[:, 1]]), torch.cat([U1[:, 2],U2[:, 2]]));\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have succesfully shown requirement *(a)* is satisfied by U1 and U2. Now, we need to check the local coordinate maps for U1 and U2, in particular, for each of them, we need to check whether on the overlap of any pair of coordinate charts, $U_{\\alpha}\\cap U_{\\beta}$ the composite map\n",
    "$$\n",
    "\\chi_{\\beta}\\circ \\chi_{\\alpha}^{-1}\\colon \\chi_{\\alpha}(\n",
    "    U_{\\alpha}\\cap U_{\\beta}\n",
    ") \\mapsto \\chi_{\\beta}(\n",
    "    U_{\\alpha}\\cap U_{\\beta}\n",
    ")\n",
    "$$\n",
    "\n",
    "is a smooth (***inifinitely differentiable***) function. \n",
    "\n",
    "So first, we need to define the local coordinate maps $\\chi_{\\alpha}$ and $\\chi_{\\beta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the coordinate charts for U_1 and U_2 as follows:\n",
    "# Let chi_alpha: U_alpha -> R^2 ~= {(x, y, 0)}, alpha = 1, 2\n",
    "# be stereographic projections from the north and south poles, respectively.\n",
    "# Then chi_1 is given by:\n",
    "# chi_1(x, y, z) = (x / (1 - z), y / (1 - z))\n",
    "# and chi_2 is given by:\n",
    "# chi_2(x, y, z) = (x / (1 + z), y / (1 + z))\n",
    "\n",
    "# We can define these coordinate charts as functions in Python.\n",
    "def chi_alpha(U: torch.Tensor, sign: int = [-1, 1]) -> torch.Tensor:\n",
    "    x, y, z = U[:, 0], U[:, 1], U[:, 2]\n",
    "    return torch.stack([x / (1 + sign * z), y / (1 + sign * z)], dim=1)\n",
    "\n",
    "chi_1 = chi_alpha(U1, alpha=-1)\n",
    "chi_2 = chi_alpha(U2, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(chi_1[:, 0], chi_1[:, 1]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(chi_2[:, 0], chi_2[:, 1]);\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local coordinate maps for the sphere $S^2$ can be defined using spherical coordinates. For a point $p = (x, y, z)$ on the sphere, we can define the local coordinate maps $\\chi_{\\alpha}$ and $\\chi_{\\beta}$ as follows:\n",
    "\n",
    "$$\n",
    "\\chi_{\\alpha}(p) = (\\theta, \\phi) = (\\arctan(y/x), \\arccos(z))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\chi_{\\beta}(p) = (\\theta', \\phi') = (\\arctan(y/x), \\pi - \\arccos(z))\n",
    "$$\n",
    "\n",
    "where $\\theta, \\theta' \\in [0, 2\\pi]$ and $\\phi, \\phi' \\in [0, \\pi]$. The local coordinate maps $\\chi_{\\alpha}$ and $\\chi_{\\beta}$ map points in $U_{\\alpha}$ and $U_{\\beta}$ respectively to points in $V_{\\alpha} = [0, 2\\pi) \\times [0, \\pi)$ and $V_{\\beta} = [0, 2\\pi) \\times (0, \\pi]$.\n",
    "\n",
    "We can now check the smoothness of the composite map $\\chi_{\\beta} \\circ \\chi_{\\alpha}^{-1}$ on the overlap $U_{\\alpha} \\cap U_{\\beta}$. Since $\\chi_{\\alpha}$ and $\\chi_{\\beta}$ are both smooth functions, their inverse functions $\\chi_{\\alpha}^{-1}$ and $\\chi_{\\beta}^{-1}$ are also smooth. Therefore, the composite map $\\chi_{\\beta} \\circ \\chi_{\\alpha}^{-1}$ is a smooth function.\n",
    "\n",
    "Finally, we need to check the third condition of the definition of a manifold. For any two distinct points $x \\in U_{\\alpha}$ and $\\tilde{x} \\in U_{\\beta}$, we need to find open subsets $W \\subset V_{\\alpha}$ and $\\tilde{W} \\subset V_{\\beta}$ such that $\\chi_{\\alpha}(x) \\in W$, $\\chi_{\\beta}(\\tilde{x}) \\in \\tilde{W}$, and $\\chi_{\\alpha}^{-1}(W) \\cap \\chi_{\\beta}^{-1}(\\tilde{W}) = \\emptyset$. This condition is satisfied because for any two distinct points on the sphere, we can always find small enough neighborhoods around these points that do not intersect.\n",
    "\n",
    "Therefore, $S^2$ is a 2-dimensional manifold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the number of points to generate\n",
    "num_points = 1000\n",
    "\n",
    "# Generate random spherical coordinates\n",
    "theta = 2 * torch.pi * torch.rand(num_points)\n",
    "phi = torch.acos(2 * torch.rand(num_points) - 1)\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "x = torch.sin(phi) * torch.cos(theta)\n",
    "y = torch.sin(phi) * torch.sin(theta)\n",
    "z = torch.cos(phi)\n",
    "\n",
    "# Convert Cartesian coordinates to parameters of the stereographic projection\n",
    "u = x / (1 - z)\n",
    "v = y / (1 - z)\n",
    "\n",
    "# Convert parameters of the stereographic projection to Cartesian coordinates\n",
    "denominator = 1 + u**2 + v**2\n",
    "x_prime = 2 * u / denominator\n",
    "y_prime = 2 * v / denominator\n",
    "z_prime = (-1 + u**2 + v**2) / denominator\n",
    "\n",
    "# Convert Cartesian coordinates to spherical coordinates\n",
    "theta_prime = torch.atan2(y_prime, x_prime)\n",
    "phi_prime = torch.acos(z_prime)\n",
    "\n",
    "# Adjust the range of theta_prime to [0, 2*pi]\n",
    "theta_prime = (theta_prime + 2 * torch.pi) % (2 * torch.pi)\n",
    "\n",
    "# Check that the original and final spherical coordinates are the same\n",
    "print(torch.allclose(theta, theta_prime, atol=1e-6))\n",
    "print(torch.allclose(phi, phi_prime, atol=1e-6))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of the Verification of the Manifold Property of $S^2$\n",
    "\n",
    "In this notebook, we have computationally verified that the 2-dimensional sphere $S^2$ is indeed a 2-dimensional manifold. We have done this by demonstrating that two different parameterizations of $S^2$ (spherical coordinates and stereographic projection) are equivalent and cover the same set $S^2$.\n",
    "\n",
    "Specifically, we have:\n",
    "\n",
    "1. Generated random points on $S^2$ using spherical coordinates.\n",
    "2. Transformed these points to the parameters of the stereographic projection.\n",
    "3. Transformed these parameters back to spherical coordinates.\n",
    "\n",
    "The fact that the original and final spherical coordinates are the same (to within a specified tolerance) confirms that the two parameterizations are equivalent and cover the same set $S^2$.\n",
    "\n",
    "This result is significant because it demonstrates that different choices of local coordinate charts can satisfy the definition of a manifold. This is a key property of manifolds and is fundamental to their study in differential geometry and related fields.\n",
    "\n",
    "## Next Steps: Exploring the Torus\n",
    "\n",
    "Having explored the manifold properties of the sphere $S^2$, we will next turn our attention to another important 2-dimensional manifold: the torus. The torus can be thought of as the Cartesian product of the circle $S^1$ with itself. In the following sections, we will explore the properties of the torus and demonstrate its manifold structure.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Torus as a 2-Dimensional Manifold\n",
    "\n",
    "The torus, often visualized as the shape of a doughnut or an inner tube, is another example of a 2-dimensional manifold. It can be thought of as the Cartesian product of the circle $S^1$ with itself, denoted as $S^1 \\times S^1$.\n",
    "\n",
    "We can parameterize the torus using two angles, $\\theta$ and $\\phi$, which correspond to rotations around the two circular directions of the torus. Given a major radius $R$ and a minor radius $r$, the parameterization in Cartesian coordinates is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x &= (R + r\\cos\\theta)\\cos\\phi \\\\\n",
    "y &= (R + r\\cos\\theta)\\sin\\phi \\\\\n",
    "z &= r\\sin\\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\theta, \\phi \\in [0, 2\\pi)$. This parameterization covers the entire torus except for a single point, which can be covered by a second parameterization.\n",
    "\n",
    "Let's generate and plot points on the torus using this parameterization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the major and minor radii\n",
    "R = 3\n",
    "r = 1\n",
    "\n",
    "# Define the number of points to generate\n",
    "num_points = 100000  # Increase the number of points\n",
    "\n",
    "# Generate random angles theta and phi\n",
    "theta = 2 * torch.pi * torch.rand(num_points)\n",
    "phi = 2 * torch.pi * torch.rand(num_points)\n",
    "\n",
    "# Calculate the Cartesian coordinates\n",
    "x = (R + r * torch.cos(theta)) * torch.cos(phi)\n",
    "y = (R + r * torch.cos(theta)) * torch.sin(phi)\n",
    "z = r * torch.sin(theta)\n",
    "\n",
    "# Plot the points on the torus\n",
    "fig = plt.figure(figsize=(10, 10))  # Increase the size of the figure\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x.numpy(), y.numpy(), z.numpy(), alpha=0.6, edgecolors='w', s=20)\n",
    "ax.set_box_aspect([1,1,1])  # Make the aspect ratio equal\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lie Groups\n",
    "\n",
    "A Lie group is a group that is also a differentiable manifold, such that the group operations (multiplication and inversion) are smooth. This means that a Lie group is a set that is equipped with a group structure, a manifold structure, and these structures are compatible in the sense that group operations are smooth functions.\n",
    "\n",
    "Let's break down the definition:\n",
    "\n",
    "1. **Group Structure:** A group is a set $G$ equipped with an operation $\\cdot: G \\times G \\rightarrow G$ (often written multiplicatively) and an inversion operation $^{-1}: G \\rightarrow G$ such that the following axioms are satisfied:\n",
    "\n",
    "   - **Closure:** For all $a, b \\in G$, the result of the operation $a \\cdot b$ is also in $G$.\n",
    "   - **Associativity:** For all $a, b, c \\in G$, the equation $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$ holds.\n",
    "   - **Identity element:** There is an element $e \\in G$ such that for every element $a \\in G$, the equations $e \\cdot a = a$ and $a \\cdot e = a$ hold.\n",
    "   - **Inverse element:** For each element $a \\in G$, there exists an element $b \\in G$ such that $a \\cdot b = e$ and $b \\cdot a = e$.\n",
    "\n",
    "2. **Manifold Structure:** As we discussed earlier, a manifold is a topological space that locally resembles Euclidean space. In the case of a Lie group, we require the manifold to be differentiable, meaning that we can do calculus on it. \n",
    "\n",
    "3. **Compatibility of Structures:** The group operations (multiplication and inversion) are required to be smooth functions when considered as maps between manifolds. More formally, if we denote the multiplication operation by $\\mu: G \\times G \\rightarrow G$ (so that $\\mu(g, h) = g \\cdot h$) and the inversion operation by $i: G \\rightarrow G$ (so that $i(g) = g^{-1}$), then $\\mu$ and $i$ are required to be smooth.\n",
    "\n",
    "An example of a Lie group is the general linear group $GL(n, \\R)$, which consists of all $n \\times n$ invertible matrices with real entries. The group operation is matrix multiplication, and the manifold structure comes from identifying each matrix with a point in $\\R^{n^2}$. The group operations are smooth functions, so $GL(n, \\R)$ is a Lie group.\n",
    "\n",
    "Another example is the circle $S^1$ with the operation of complex multiplication. Each point on the circle can be identified with a complex number of absolute value 1, and multiplication of such numbers is a smooth operation.\n",
    "\n",
    "Let's consider the general linear group $GL(2, \\R)$ for simplicity. This group consists of all $2 \\times 2$ invertible matrices with real entries. A general element of $GL(2, \\R)$ can be written as:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $a$, $b$, $c$, and $d$ are real numbers and $ad - bc \\neq 0$ (the condition for the matrix to be invertible).\n",
    "\n",
    "The group operation is matrix multiplication, and the inverse of a matrix is given by:\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, let's consider some subgroups of $GL(2, \\R)$:\n",
    "\n",
    "1. **Orthogonal Group $O(2)$:** This is the group of $2 \\times 2$ matrices that preserve the Euclidean norm, i.e., $AA^T = A^TA = I$. The determinant of such matrices is either 1 or -1. A general element of $O(2)$ can be written as:\n",
    "\n",
    "    $$\n",
    "    O = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\ \\sin \\theta & -\\cos \\theta \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    where $\\theta$ is a real number.\n",
    "\n",
    "2. **Special Orthogonal Group $SO(2)$:** This is the subgroup of $O(2)$ consisting of matrices with determinant 1. These are rotations in the plane. A general element of $SO(2)$ can be written as:\n",
    "\n",
    "    $$\n",
    "    SO = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    where $\\theta$ is a real number.\n",
    "\n",
    "These subgroups are also Lie groups, as they are groups and differentiable manifolds, and the group operations are smooth. They are also examples of compact Lie groups, as they are closed and bounded subsets of $\\R^{2 \\times 2}$.\n",
    "\n",
    "Sure, let's go through the matrix multiplication step by step. \n",
    "\n",
    "Matrix multiplication is a binary operation that takes a pair of matrices, and produces another matrix. For $2 \\times 2$ matrices, the multiplication is defined as follows:\n",
    "\n",
    "If we have two matrices $A$ and $B$ in $GL(2, \\R)$, where\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\quad \\text{and} \\quad B = \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "their product $AB$ is given by\n",
    "\n",
    "$$\n",
    "AB = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix} = \\begin{bmatrix} ae + bg & af + bh \\\\ ce + dg & cf + dh \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This operation is associative, meaning that for any three matrices $A\\$, $B$, and $C$ in $GL(2, \\R)$, we have $(AB)C = A(BC)$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider two specific matrices in $GL(2, \\R)$:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\quad \\text{and} \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Their product is given by\n",
    "\n",
    "$$\n",
    "AB = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1*5 + 2*7 & 1*6 + 2*8 \\\\ 3*5 + 4*7 & 3*6 + 4*8 \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the product of $A$ and $B$ is,\n",
    "$$\n",
    "\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
    "$$\n",
    "demonstrating the closure property of the group.\n",
    "\n",
    "\n",
    "Below, we show that the exponentiated value of simple 2x2 generator matrices is equal to the group of rotation matrices, a simple result with extremely significant implications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to generate a skew-symmetric matrix\n",
    "def skew_symmetric(theta):\n",
    "    return theta * torch.tensor([[0, -1], [1, 0]])\n",
    "\n",
    "# Define a vector\n",
    "v = torch.tensor([1.0, 0.0])\n",
    "\n",
    "# Generate a sequence of skew-symmetric matrices and compute their matrix exponentials\n",
    "thetas = torch.linspace(0, 0.1, 10)\n",
    "skew_symmetric_matrices = [skew_symmetric(theta) for theta in thetas]\n",
    "rotation_matrices = [torch.linalg.matrix_exp(X) for X in skew_symmetric_matrices]\n",
    "\n",
    "# Apply the rotation matrices to the vector\n",
    "v_rotated = [R @ v for R in rotation_matrices]\n",
    "\n",
    "# Plot the original and rotated vectors\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.quiver(*v, angles='xy', scale_units='xy', scale=1, color='r')\n",
    "for v_r in v_rotated:\n",
    "    plt.quiver(*v_r, angles='xy', scale_units='xy', scale=1, color='b', alpha=0.2)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can see that the symmetry groups of transformations of objects in 2D space can be represented by the group of rotation matrices, which can be generated by 2x2 real matrices.\n",
    "\n",
    "Consider the simplest case of a 2x2 generator matrix, also known as a skew-symmetric matrix:\n",
    "\n",
    "$$\n",
    "G = \\begin{bmatrix} 0 & -\\theta \\\\ \\theta & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\theta$ is a scalar. You can observe that this matrix is skew-symmetric, i.e., $G^T = -G$.\n",
    "\n",
    "Now, let's exponentiate this matrix $G$ using the matrix exponential function $\\exp$. The matrix exponential is a power series defined as:\n",
    "\n",
    "$$\n",
    "\\exp(G) = I + G + \\frac{1}{2!} G^2 + \\frac{1}{3!} G^3 + \\dots = \\sum_{k=0}^{\\infty} \\frac{1}{k!} G^k\n",
    "$$\n",
    "\n",
    "We can compute the first few powers of $G$:\n",
    "\n",
    "$$\n",
    "G^0 = I, \\quad G^1 = G, \\quad G^2 = \\begin{bmatrix} -\\theta^2 & 0 \\\\ 0 & -\\theta^2 \\end{bmatrix}, \\quad G^3 = -\\theta G, \\quad G^4 = \\theta^2 I, \\quad \\dots\n",
    "$$\n",
    "\n",
    "Now, we plug these matrix powers into the power series and separate the even and odd terms:\n",
    "\n",
    "$$\n",
    "\\exp(G) = (I + \\frac{1}{2!} G^2 + \\frac{1}{4!} G^4 + \\dots) + (G + \\frac{1}{3!} G^3 + \\dots) = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As a result, the matrix exponential of the 2x2 skew-symmetric matrix generates the special orthogonal group $SO(2)$, which is the group of rotation matrices.\n",
    "\n",
    "These results have significant implications for understanding how to apply Lie groups and matrix exponential to deep learning models, such as Transformer-based architectures. By leveraging the properties of exponentiated generator matrices and understanding the underlying structure, researchers can design models that are more robust and efficient when handling different types of data. Moreover, the idea of matrix exponentiation facilitates a natural way to interpolate between different network parameters when considering weight sharing, encouraging smooth behavior.\n",
    "\n",
    "### How does this relate to neural networks?\n",
    "\n",
    "Consider a sequence of input data $x_1, x_2, \\dots, x_n$. These data points can be visualized in a high-dimensional space. One of the main components of the Transformer architecture is the self-attention mechanism, which computes an attention score for each element within a sequence. The attention mechanism represents relations between elements in the sequence geometrically, using dot products between those elements in the high-dimensional space. \n",
    "\n",
    "By applying continuous transformations to this high-dimensional space, one could potentially extract additional information about the structures embedded in the input data. Lie groups play an important role in this regard. A continuous transformation in a high-dimensional space can be represented as an action of a Lie group on the manifold of data points. In practice, elements of a Lie group are given by the exponentiation of Lie algebra elements, which are closely related to matrix exponentials.\n",
    "\n",
    "Suppose we have a Lie group in the form of a matrix exponential, as shown before:\n",
    "\n",
    "$$\n",
    "\\exp(G) = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying this transformation to the input data in the high-dimensional space would result in a new representation of the data points. The transformed data points can be further used as input to a Transformer layer. This transformed representation might allow the attention mechanism to focus on different aspects of the input data and can potentially capture more complex relational structures present.\n",
    "\n",
    "However, this approach has not yet been fully explored in the Transformer architectures, and most research has focused on finding more efficient ways to apply the attention mechanism, rather than incorporating geometric transformations explicitly. One potential direction for future research could be to consider the effect of applying transformations from special types of Lie groups on attention scores and relevance of input data points, and observe the impact this might have on model performance.\n",
    "\n",
    "That being said, directly applying Lie group transformations as shown might not be the most natural or efficient way to incorporate the power of Lie groups and their symmetries into Transformer-based architectures. A more elegant approach would be to explore how Lie groups could be integrated into the design of Transformer networks inherently.\n",
    "\n",
    "One possibility is to incorporate equivariance to Lie group actions into the self-attention mechanism. The principle of equivariance implies that the output of a function should transform in the same way as the input under a given transformation. In this context, it means that the attention mechanism should be designed such that it remains unchanged under the action of a Lie group transformation applied to input data.\n",
    "\n",
    "To incorporate this idea into the self-attention mechanism, we need to rethink the computation of attention scores. Currently, attention scores are computed using a dot product between the query, key, and value vectors. Instead, we could design an attention mechanism that computes the scores after some consideration of the Lie group transformations.\n",
    "\n",
    "For example, considering a Lie group of rotations, the design could compute attention scores in a rotation-invariant manner. This would involve redefining the computation of attention scores as the similarity between input element embeddings up to rotations (Lie group actions), rather than solely relying on dot products, which are not rotation-invariant\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll use the `torchtext` library to download the Wikitext-2 dataset, which is a collection of Wikipedia articles. We'll preprocess the text data and create a data loader to be used during training. Then, we will build a simple language model using the previously defined rotation-invariant transformer layer and train the model on the dataset.\n",
    "\n",
    "### 1. First, make sure you have the `torchtext` library installed. You can install it with pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: torchdata==0.6.1 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torchtext) (0.6.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torchtext) (2.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torchtext) (1.24.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch==2.0.1->torchtext) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch==2.0.1->torchtext) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch==2.0.1->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch==2.0.1->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from torchdata==0.6.1->torchtext) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->torchtext) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->torchtext) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from jinja2->torch==2.0.1->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\envs\\gpt\\lib\\site-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Next, let's download and preprocess the Wikitext-2 dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install portalocker>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Download the dataset as a zip file\n",
    "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "response = requests.get(url)\n",
    "zip_content = response.content\n",
    "\n",
    "# Extract the dataset from the zip file\n",
    "with zipfile.ZipFile(io.BytesIO(zip_content), 'r') as zip_ref:\n",
    "    zip_ref.extractall('wikitext-2')\n",
    "\n",
    "# Load data from files\n",
    "with open('wikitext-2/wikitext-2/wiki.train.tokens', 'r', encoding='utf-8') as f:\n",
    "    train_data_raw = f.read()\n",
    "with open('wikitext-2/wikitext-2/wiki.valid.tokens', 'r', encoding='utf-8') as f:\n",
    "    valid_data_raw = f.read()\n",
    "with open('wikitext-2/wikitext-2/wiki.test.tokens', 'r', encoding='utf-8') as f:\n",
    "    test_data_raw = f.read()\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = tokenizer(train_data_raw)\n",
    "valid_iter = tokenizer(valid_data_raw)\n",
    "test_iter = tokenizer(test_data_raw)\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = build_vocab_from_iterator(train_iter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Preprocess the data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess_data(data_iter, vocab, device):\n",
    "    tensor_data = torch.tensor([vocab[token] for token in data_iter], dtype=torch.long, device=device)\n",
    "    # Insert <bos> and <eos> tokens to the beginning and end of the sequence\n",
    "    yield torch.cat([torch.tensor([vocab['<bos>']], device=device), tensor_data, torch.tensor([vocab['<eos>']], device=device)])\n",
    "\n",
    "train_data = list(preprocess_data(train_iter, vocab, device))\n",
    "valid_data = list(preprocess_data(valid_iter, vocab, device))\n",
    "test_data = list(preprocess_data(test_iter, vocab, device))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. Create a data loader for the language modeling task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    total_seq_len = sum([len(x) for x in data])\n",
    "    num_batch_elements = total_seq_len // batch_size\n",
    "    \n",
    "    # Concatenate and reshape data into batch_size columns\n",
    "    batched_data = torch.cat(data)\n",
    "    batched_data = batched_data.narrow(0, 0, num_batch_elements * batch_size)\n",
    "    batched_data = batched_data.view(batch_size, -1).t().contiguous()\n",
    "    return batched_data\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(batchify(train_data, batch_size), batch_size=batch_size)\n",
    "valid_loader = DataLoader(batchify(valid_data, eval_batch_size), batch_size=eval_batch_size)\n",
    "test_loader = DataLoader(batchify(test_data, eval_batch_size), batch_size=eval_batch_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. Build a simple language model using the rotation-invariant transformer layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RotationInvariantMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def euclidean_distance(self, x, y):\n",
    "        return torch.sum((x - y) ** 2, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2).unsqueeze(3)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2).unsqueeze(2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2).unsqueeze(2)\n",
    "        \n",
    "        scores = self.euclidean_distance(query, key)\n",
    "        scores = - torch.sqrt(scores)\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        print(f\"query shape: {query.shape}\")  # Add this line\n",
    "        print(f\"key shape: {key.shape}\")  # Add this line\n",
    "        print(f\"value shape: {value.shape}\")  # Add this line\n",
    "        print(f\"weights shape: {weights.shape}\")  # Add this line\n",
    "        \n",
    "        attention = torch.einsum(\"bnqd,bnqd->bnqd\", weights, value).contiguous()\n",
    "\n",
    "        print(f\"attention shape: {attention.shape}\")  # Add this line\n",
    "\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out_linear(attention)\n",
    "\n",
    "class RotationInvariantTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = RotationInvariantMultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-Attnetion\n",
    "        attn_out = self.multi_head_attention(x)\n",
    "        x = self.norm1(attn_out + x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Position-wise Feedforward\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(ff_out + x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class RotationInvariantTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.layers = nn.ModuleList([RotationInvariantTransformerLayer(d_model, num_heads) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = RotationInvariantTransformer(input_dim=d_model, d_model=d_model, num_heads=num_heads, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"x dimensions in LanguageModel: {x.shape}\")\n",
    "        embedded = self.embedding(x)\n",
    "        transformer_out = self.transformer(embedded)\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "seq_len = 35\n",
    "\n",
    "model = LanguageModel(vocab_size, d_model, num_heads, num_layers, seq_len).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5. Now, let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x dimensions in LanguageModel: torch.Size([63, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, criterion)\n\u001b[0;32m     41\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_loader, criterion)\n\u001b[0;32m     42\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Train loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation loss: \u001b[39m\u001b[39m{\u001b[39;00mvalid_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m      9\u001b[0m input_data \u001b[39m=\u001b[39m batch[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m     10\u001b[0m target_data \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m:, :]\n\u001b[1;32m---> 11\u001b[0m output \u001b[39m=\u001b[39m model(input_data)  \u001b[39m# Changed to use LanguageModel\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), target_data\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     13\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 98\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx dimensions in LanguageModel: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m---> 98\u001b[0m transformer_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(embedded)\n\u001b[0;32m     99\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(transformer_out)\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 81\u001b[0m, in \u001b[0;36mRotationInvariantTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 81\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x)\n\u001b[0;32m     82\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m     83\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        input_data = batch[:-1, :]\n",
    "        target_data = batch[1:, :]\n",
    "        output = model(input_data)  # Changed to use LanguageModel\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), target_data.view(-1))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return running_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_data = batch[:-1, :]\n",
    "            target_data = batch[1:, :]\n",
    "            output = model(input_data)  # Changed to use LanguageModel\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), target_data.view(-1))\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(iterator)\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    print(f'Epoch: {epoch}, Train loss: {train_loss:.3f}, Validation loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "6. Lastly, we'll implement a `generate()` method to generate new sequences using the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(model, start_text, generate_len=30, temperature=0.8):\n",
    "    model.eval()\n",
    "    input_data = torch.tensor([vocab[token] for token in tokenizer(start_text)], dtype=torch.long, device=device).unsqueeze(1)\n",
    "    \n",
    "    hidden = None\n",
    "    generated_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(generate_len):\n",
    "            output = model(input_data)\n",
    "            output = output / temperature\n",
    "            output = torch.exp(output)\n",
    "            probs = output[-1, :].squeeze().cpu()\n",
    "\n",
    "            # Sample from the output distribution\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token = vocab.itos[next_token_idx]\n",
    "\n",
    "            # Append the generated token to the existing sequence and update input data\n",
    "            generated_text += \" \" + next_token\n",
    "            input_data = torch.cat([input_data, torch.tensor([[next_token_idx]], device=device)], dim=0)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "print(generate(model, \"The history of\", generate_len=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This example code demonstrates how to train a language model using a custom rotation-invariant transformer on the Wikitext-2 dataset and generate new text sequences using the trained model. Keep in mind that this model is relatively simplistic and has not been fine-tuned for optimal performance. Hyperparameter adjustments or deeper exploration of structures related to transformation-equivariant attention may be required to achieve high-quality results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

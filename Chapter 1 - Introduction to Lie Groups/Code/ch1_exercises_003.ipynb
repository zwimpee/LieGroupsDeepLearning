{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nanoGPT' already exists and is not an empty directory.\n",
      "INFO:root:Fetching data...\n",
      "INFO:root:Tokenizing data...\n",
      "INFO:root:Building vocab...\n",
      "INFO:root:Processing data...\n",
      "INFO:root:Vocab size: 28785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 33.65M\n",
      "number of parameters: 33.65M\n",
      "Training GPT\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch 0: Loss=10.711923599243164\n",
      "INFO:root:Batch 100: Loss=0.10604323446750641\n",
      "INFO:root:Batch 200: Loss=0.1528988778591156\n",
      "INFO:root:Batch 300: Loss=0.08255700021982193\n",
      "INFO:root:Batch 400: Loss=0.0793856829404831\n",
      "INFO:root:Batch 500: Loss=nan\n",
      "INFO:root:Batch 600: Loss=nan\n",
      "INFO:root:Batch 700: Loss=0.049694448709487915\n",
      "INFO:root:Batch 800: Loss=nan\n",
      "INFO:root:Batch 900: Loss=0.0017437603091821074\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 525\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    524\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 525\u001b[0m     train_loss \u001b[39m=\u001b[39m train(\n\u001b[0;32m    526\u001b[0m         model,\n\u001b[0;32m    527\u001b[0m         train_loader,\n\u001b[0;32m    528\u001b[0m         optimizer, \n\u001b[0;32m    529\u001b[0m         criterion, \n\u001b[0;32m    530\u001b[0m         \u001b[39m#scaler\u001b[39;49;00m\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidating epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    533\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_loader)\n",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion, scaler)\u001b[0m\n\u001b[0;32m     98\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[0;32m    100\u001b[0m \u001b[39m# scaler.scale(loss).backward()\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m# scaler.step(optimizer)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m# scaler.update()\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    105\u001b[0m \u001b[39m#print(model.transformer.h[0].attn.c_attn.weight.grad)  # Access weights gradient of the first layer's attention module\u001b[39;00m\n\u001b[0;32m    106\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\gpt\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import platform\n",
    "from typing import Tuple, List, Union, Dict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Set the logging level to DEBUG\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "!git clone https://github.com/karpathy/nanoGPT.git\n",
    "from nanoGPT.model import GPTConfig, GPT, MLP\n",
    "\n",
    "# Functions\n",
    "\n",
    "# Define a function to move tensors to the GPU\n",
    "def move_to_device(data, device):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, list):\n",
    "        return [move_to_device(d, device) for d in data]\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: move_to_device(v, device) for k, v in data.items()}\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def batchify(data: List[torch.Tensor], batch_size: int) -> DataLoader:\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# a function to fetch and process data.\n",
    "def fetch_and_process_data() -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor], Vocab]:\n",
    "    logging.info(\"Fetching data...\")\n",
    "    url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "    response = requests.get(url)\n",
    "    zip_content = response.content\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(zip_content), 'r') as zip_ref:\n",
    "        zip_ref.extractall('wikitext-2')\n",
    "\n",
    "    with open('wikitext-2/wikitext-2/wiki.train.tokens', 'r', encoding='utf-8') as f:\n",
    "        train_data_raw = f.read()\n",
    "    with open('wikitext-2/wikitext-2/wiki.valid.tokens', 'r', encoding='utf-8') as f:\n",
    "        valid_data_raw = f.read()\n",
    "    with open('wikitext-2/wikitext-2/wiki.test.tokens', 'r', encoding='utf-8') as f:\n",
    "        test_data_raw = f.read()\n",
    "\n",
    "    logging.info(\"Tokenizing data...\")\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    train_iter = tokenizer(train_data_raw)\n",
    "    valid_iter = tokenizer(valid_data_raw)\n",
    "    test_iter = tokenizer(test_data_raw)\n",
    "\n",
    "    logging.info(\"Building vocab...\")\n",
    "    vocab = build_vocab_from_iterator([train_iter, valid_iter, test_iter], specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "    logging.info(\"Processing data...\")\n",
    "    train_data = [torch.tensor([vocab[token] for token in tokenizer(sample)], dtype=torch.long) for sample in train_data_raw.split()]\n",
    "    valid_data = [torch.tensor([vocab[token] for token in tokenizer(sample)], dtype=torch.long) for sample in valid_data_raw.split()]\n",
    "    test_data = [torch.tensor([vocab[token] for token in tokenizer(sample)], dtype=torch.long) for sample in test_data_raw.split()]\n",
    "\n",
    "    return train_data, valid_data, test_data, vocab\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader: DataLoader, optimizer: optim.Optimizer, criterion, scaler: GradScaler = None) -> float:\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for ii, batch in enumerate(dataloader):\n",
    "        x = batch[:, :-1].to(device)\n",
    "        y = batch[:, 1:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        \n",
    "        loss.backward()\n",
    "        #print(model.transformer.h[0].attn.c_attn.weight.grad)  # Access weights gradient of the first layer's attention module\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if ii % 100 == 0:\n",
    "            logging.info(f\"Batch {ii}: Loss={loss.item()}\")\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ii, batch in enumerate(dataloader):\n",
    "            batch = move_to_device(batch, device)  # Move batch to GPU\n",
    "\n",
    "            x = batch[:, :-1]\n",
    "            y = batch[:, 1:]\n",
    "\n",
    "            logits, loss = model(x, y)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if ii % 100 == 0:\n",
    "                logging.info(f\"Batch {ii}: Validation Loss={loss.item()}\")\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Define a generalizable class for loss functions\n",
    "\n",
    "# TODO: Implement the definition for custom loss function definitions\n",
    "class Loss(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\n",
    "def new_rielu(x):\n",
    "    \"\"\"\n",
    "    Modified implementation of the BERT GELU to conform to rotational symmetries\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "# Models\n",
    "class RotationInvariantLayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class RotationInvariantAttention(nn.Module):  # modified from nanoGPT causal self-attention\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
    "\n",
    "        # reshape q, k, v for multihead attention\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Calculate attention scores as negative Euclidean distance\n",
    "        att_scores = -torch.sqrt(torch.sum((q.unsqueeze(-2) - k.unsqueeze(-3)) ** 2, dim=-1))\n",
    "\n",
    "        # Scale the attention scores by the square root of their dimensionality\n",
    "        scaling_factor = q.size(-1) ** 0.5  # assuming q and k have the same dimensionality\n",
    "        att_scores = att_scores / scaling_factor\n",
    "\n",
    "        # Apply softmax to obtain attention weights\n",
    "        att_weights = F.softmax(att_scores, dim=-1)\n",
    "\n",
    "        # calculate weighted sum of value vectors\n",
    "        y = torch.matmul(att_weights.unsqueeze(-2), v).squeeze(-2)\n",
    "\n",
    "        # re-assemble all head outputs side by side\n",
    "        y = y.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class RotationallyInvariantMLP(nn.Module):  # copied directly from nanoGPT causal self-attention\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = new_gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class RotationallyInvariantBlock(nn.Module): # modified from nanoGPT causal self-attention\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = RotationInvariantLayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = RotationInvariantAttention(config)\n",
    "        self.ln_2 = RotationInvariantLayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class RotationallyInvariantGPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "class RotationallyInvariantGPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([RotationallyInvariantBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = RotationInvariantLayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "# Configurations and\n",
    "\n",
    "train_data, valid_data, test_data, vocab = fetch_and_process_data()\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "logging.info(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 128\n",
    "\n",
    "train_loader = batchify(train_data, batch_size)\n",
    "valid_loader = batchify(valid_data, eval_batch_size)\n",
    "test_loader = batchify(test_data, eval_batch_size)\n",
    "\n",
    "# Device assignment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Hyperparameter setting\n",
    "\n",
    "# Hyperparameters (GPTConfig)\n",
    "d_model = 512     # Increase the number of embedding dimensions\n",
    "num_heads = 8     # Increase the number of attention heads\n",
    "num_layers = 6    # Increase the number of transformer layers\n",
    "block_size = 512  # Increase the sequence length\n",
    "dropout = 0.2      # Slightly increase the dropout rate\n",
    "bias = True        # Use a bias term in linear layers\n",
    "\n",
    "\n",
    "gpt_config = GPTConfig(vocab_size=vocab_size, n_embd=d_model, n_head=num_heads, n_layer=num_layers, block_size=block_size, dropout=dropout, bias=bias)\n",
    "rigpt_config = RotationallyInvariantGPTConfig(vocab_size=vocab_size, n_embd=d_model, n_head=num_heads, n_layer=num_layers, block_size=block_size, dropout=dropout, bias=bias)\n",
    "\n",
    "# Initialize models\n",
    "gpt = GPT(gpt_config).to(device)\n",
    "rigpt = RotationallyInvariantGPT(rigpt_config).to(device)\n",
    "\n",
    "# Training settings\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "batch_size = 16\n",
    "eval_batch_size = 32\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_gpt = optim.Adam(gpt.parameters(), lr=lr)\n",
    "optimizer_rigpt = optim.Adam(rigpt.parameters(), lr=lr)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize GradScaler\n",
    "#scaler = GradScaler()\n",
    "\n",
    "# Train and evaluate both models\n",
    "for model, optimizer, model_name in [(gpt, optimizer_gpt, 'GPT'), (rigpt, optimizer_rigpt, 'RotationallyInvariantGPT')]:\n",
    "    print(f\"Training {model_name}\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        train_loss = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer, \n",
    "            criterion, \n",
    "            #scaler\n",
    "        )\n",
    "        print(f\"Validating epoch {epoch}\")\n",
    "        valid_loss = evaluate(model, valid_loader)\n",
    "        print(f'{model_name} - Epoch: {epoch}, Train loss: {train_loss:.3f}, Validation loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
